package cluster

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/hashicorp/terraform-exec/tfexec"
	"github.com/pkg/errors"
	"github.com/sirupsen/logrus"
	"gopkg.in/yaml.v2"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/wait"
	clusterv1 "sigs.k8s.io/cluster-api/api/v1beta1"
	utilkubeconfig "sigs.k8s.io/cluster-api/util/kubeconfig"
	"sigs.k8s.io/controller-runtime/pkg/client"

	"github.com/openshift/installer/pkg/asset"
	"github.com/openshift/installer/pkg/asset/cluster/aws"
	"github.com/openshift/installer/pkg/asset/cluster/azure"
	"github.com/openshift/installer/pkg/asset/cluster/openstack"
	"github.com/openshift/installer/pkg/asset/ignition/bootstrap"
	"github.com/openshift/installer/pkg/asset/ignition/machine"
	"github.com/openshift/installer/pkg/asset/installconfig"
	awsconfig "github.com/openshift/installer/pkg/asset/installconfig/aws"
	"github.com/openshift/installer/pkg/asset/kubeconfig"
	"github.com/openshift/installer/pkg/asset/machines"
	"github.com/openshift/installer/pkg/asset/manifests"
	"github.com/openshift/installer/pkg/asset/password"
	"github.com/openshift/installer/pkg/asset/quota"
	"github.com/openshift/installer/pkg/cluster-api/system"
	infra "github.com/openshift/installer/pkg/infrastructure/platform"
	"github.com/openshift/installer/pkg/metrics/timer"
	"github.com/openshift/installer/pkg/terraform"
	typesaws "github.com/openshift/installer/pkg/types/aws"
	typesazure "github.com/openshift/installer/pkg/types/azure"
	typesopenstack "github.com/openshift/installer/pkg/types/openstack"
	typesvsphere "github.com/openshift/installer/pkg/types/vsphere"
)

const (
	ClusterAPINamespace = "openshift-cluster-api-guests"
)

var (
	// InstallDir is the directory containing install assets.
	InstallDir string
)

// Cluster uses the terraform executable to launch a cluster
// with the given terraform tfvar and generated templates.
type Cluster struct {
	FileList []*asset.File
}

var _ asset.WritableAsset = (*Cluster)(nil)

// Name returns the human-friendly name of the asset.
func (c *Cluster) Name() string {
	return "Cluster"
}

// Dependencies returns the direct dependency for launching
// the cluster.
func (c *Cluster) Dependencies() []asset.Asset {
	return []asset.Asset{
		&installconfig.ClusterID{},
		&installconfig.InstallConfig{},
		// PlatformCredsCheck, PlatformPermsCheck and PlatformProvisionCheck
		// perform validations & check perms required to provision infrastructure.
		// We do not actually use them in this asset directly, hence
		// they are put in the dependencies but not fetched in Generate.
		&installconfig.PlatformCredsCheck{},
		&installconfig.PlatformPermsCheck{},
		&installconfig.PlatformProvisionCheck{},
		&quota.PlatformQuotaCheck{},
		&TerraformVariables{},
		&password.KubeadminPassword{},
		// Cluster API resources from here:
		&manifests.ClusterAPI{},
		&machines.CAPIMachine{},
		&bootstrap.Bootstrap{},
		&machine.Master{},
		&kubeconfig.AdminClient{},
		&kubeconfig.AdminInternalClient{},
	}
}

// Generate launches the cluster and generates the terraform state file on disk.
func (c *Cluster) Generate(parents asset.Parents) (err error) {
	if InstallDir == "" {
		logrus.Fatalf("InstallDir has not been set for the %q asset", c.Name())
	}

	clusterID := &installconfig.ClusterID{}
	installConfig := &installconfig.InstallConfig{}
	terraformVariables := &TerraformVariables{}
	parents.Get(clusterID, installConfig, terraformVariables)

	if fs := installConfig.Config.FeatureSet; strings.HasSuffix(string(fs), "NoUpgrade") {
		logrus.Warnf("FeatureSet %q is enabled. This FeatureSet does not allow upgrades and may affect the supportability of the cluster.", fs)
	}

	if installConfig.Config.Platform.None != nil {
		return errors.New("cluster cannot be created with platform set to 'none'")
	}

	if installConfig.Config.BootstrapInPlace != nil {
		return errors.New("cluster cannot be created with bootstrapInPlace set")
	}

	platform := installConfig.Config.Platform.Name()
	switch platform {
	case typesaws.Name, typesazure.Name, typesvsphere.Name:
		return c.generateClusterAPI(parents, installConfig, clusterID, terraformVariables)
	default:
		return c.generateTerraform(installConfig, clusterID, terraformVariables, platform)
	}
}

// Files returns the FileList generated by the asset.
func (c *Cluster) Files() []*asset.File {
	return c.FileList
}

// Load returns error if the tfstate file is already on-disk, because we want to
// prevent user from accidentally re-launching the cluster.
func (c *Cluster) Load(f asset.FileFetcher) (found bool, err error) {
	matches, err := filepath.Glob("terraform(.*)?.tfstate")
	if err != nil {
		return true, err
	}
	if len(matches) != 0 {
		return true, errors.Errorf("terraform state files alread exist.  There may already be a running cluster")
	}

	return false, nil
}

func (c *Cluster) generateClusterAPI(parents asset.Parents, installConfig *installconfig.InstallConfig, clusterID *installconfig.ClusterID, terraformVariables *TerraformVariables) error {
	capiManifests := &manifests.ClusterAPI{}
	capiMachines := &machines.CAPIMachine{}
	bootstrapIgnAsset := &bootstrap.Bootstrap{}
	masterIgnAsset := &machine.Master{}
	clusterKubeconfigAsset := &kubeconfig.AdminClient{}

	parents.Get(
		capiManifests,
		capiMachines,
		bootstrapIgnAsset,
		masterIgnAsset,
		clusterKubeconfigAsset,
	)

	platform := installConfig.Config.Platform.Name()

	// vSphere has pre-reqs that need to be executed
	switch platform {
	case typesvsphere.Name:
		tfvarsFiles := []*asset.File{}
		for _, file := range terraformVariables.Files() {
			tfvarsFiles = append(tfvarsFiles, file)
		}

		provider := infra.ProviderForPlatform(platform)
		_, err := provider.Provision(InstallDir, tfvarsFiles)
		if err != nil {
			return err
		}
	}

	// Only need the objects--not the files.
	manifests := []client.Object{}
	for _, m := range capiManifests.Manifests {
		manifests = append(manifests, m.Object)
	}
	manifests = append(manifests, capiMachines.Machines...)

	// Run the CAPI system.
	capiSystem := &system.System{}
	if err := capiSystem.Run(clusterID, installConfig); err != nil {
		return errors.Wrap(err, "failed to run cluster api system")
	}

	// Grab the client.
	cl := capiSystem.Client

	// Create the namespace for the cluster.
	ns := &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: ClusterAPINamespace,
		},
	}
	if err := cl.Create(context.Background(), ns); err != nil && !apierrors.IsAlreadyExists(err) {
		return fmt.Errorf("failed to create namespace: %w", err)
	}

	// Gather the ignition files, and store them in a secret.
	{
		masterIgn := string(masterIgnAsset.Files()[0].Data)
		bootstrapIgn, err := injectInstallInfo(bootstrapIgnAsset.Files()[0].Data)
		if err != nil {
			return errors.Wrap(err, "unable to inject installation info")
		}
		manifests = append(manifests,
			&corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Name:      fmt.Sprintf("%s-%s", clusterID.InfraID, "master"),
					Namespace: ns.Name,
					Labels: map[string]string{
						"cluster.x-k8s.io/cluster-name": clusterID.InfraID,
					},
				},
				Data: map[string][]byte{
					"format": []byte("ignition"),
					"value":  []byte(masterIgn),
				},
			},
			&corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Name:      fmt.Sprintf("%s-%s", clusterID.InfraID, "bootstrap"),
					Namespace: ns.Name,
					Labels: map[string]string{
						"cluster.x-k8s.io/cluster-name": clusterID.InfraID,
					},
				},
				Data: map[string][]byte{
					"format": []byte("ignition"),
					"value":  []byte(bootstrapIgn),
				},
			},
		)
	}

	// Create all the manifests and store them.
	for _, m := range manifests {
		m.SetNamespace(ns.Name)
		if err := cl.Create(context.Background(), m); err != nil {
			return fmt.Errorf("failed to create manifest: %w", err)
		}
		logrus.Infof("Created manifest %+T, namespace=%s name=%s", m, m.GetNamespace(), m.GetName())
	}

	// Pass cluster kubeconfig and store it in; this is usually the role of a bootstrap provider.
	{
		key := client.ObjectKey{
			Name:      clusterID.InfraID,
			Namespace: ns.Name,
		}
		cluster := &clusterv1.Cluster{}
		if err := cl.Get(context.Background(), key, cluster); err != nil {
			return err
		}
		// Create the secret.
		clusterKubeconfig := clusterKubeconfigAsset.Files()[0].Data
		secret := utilkubeconfig.GenerateSecret(cluster, clusterKubeconfig)
		if err := cl.Create(context.Background(), secret); err != nil {
			return err
		}
	}

	{
		var cluster *clusterv1.Cluster
		if err := wait.ExponentialBackoff(wait.Backoff{
			Duration: time.Second * 10,
			Factor:   float64(1.5),
			Steps:    32,
		}, func() (bool, error) {
			c := &clusterv1.Cluster{}
			if err := cl.Get(context.Background(), client.ObjectKey{
				Name:      clusterID.InfraID,
				Namespace: ns.Name,
			}, c); err != nil {
				if apierrors.IsNotFound(err) {
					return false, nil
				}
				return false, err
			}
			cluster = c
			return cluster.Spec.ControlPlaneEndpoint.IsValid(), nil
		}); err != nil {
			return err
		}
		if cluster == nil {
			return errors.New("error occurred during load balancer ready check")
		}
		if cluster.Spec.ControlPlaneEndpoint.Host == "" {
			return errors.New("control plane endpoint is not set")
		}

		// TODO: Move this to a separate asset.
		// The endpoint is available in:
		// cluster.Spec.ControlPlaneEndpoint.Host
		ssn, err := installConfig.AWS.Session(context.TODO())
		if err != nil {
			return fmt.Errorf("failed to create session: %w", err)
		}
		client := awsconfig.NewClient(ssn)
		r53cfg := awsconfig.GetR53ClientCfg(ssn, "")
		//awsconfig.ValidateForProvisioning(client, ic.Config, ic.AWS)
		err = client.CreateOrUpdateRecord(installConfig.Config, cluster.Spec.ControlPlaneEndpoint.Host, r53cfg)
		if err != nil {
			return fmt.Errorf("failed to create route53 records: %w", err)
		}
		logrus.Infof("Created Route53 records to control plane load balancer.")
	}

	// For each manifest we created, retrieve it and store it in the asset.
	for _, m := range manifests {
		key := client.ObjectKey{
			Name:      m.GetName(),
			Namespace: m.GetNamespace(),
		}
		if err := cl.Get(context.Background(), key, m); err != nil {
			return fmt.Errorf("failed to get manifest: %w", err)
		}

		gvk, err := cl.GroupVersionKindFor(m)
		if err != nil {
			return fmt.Errorf("failed to get GVK for manifest: %w", err)
		}
		fileName := fmt.Sprintf("%s-%s-%s.yaml", gvk.Kind, m.GetNamespace(), m.GetName())
		objData, err := yaml.Marshal(m)
		if err != nil {
			errMsg := fmt.Sprintf("failed to create infrastructure manifest %s from InstallConfig", fileName)
			return errors.Wrapf(err, errMsg)
		}
		c.FileList = append(c.FileList, &asset.File{
			Filename: fileName,
			Data:     objData,
		})
	}

	logrus.Infof("Cluster API resources have been created. Waiting for cluster to become ready...")
	return nil
}

func (c *Cluster) generateTerraform(installConfig *installconfig.InstallConfig, clusterID *installconfig.ClusterID, terraformVariables *TerraformVariables, platform string) error {
	if azure := installConfig.Config.Platform.Azure; azure != nil && azure.CloudName == typesazure.StackCloud {
		platform = typesazure.StackTerraformName
	}

	logrus.Infof("Creating infrastructure resources...")
	switch platform {
	case typesaws.Name:
		if err := aws.PreTerraform(context.TODO(), clusterID.InfraID, installConfig); err != nil {
			return err
		}
	case typesazure.Name, typesazure.StackTerraformName:
		if err := azure.PreTerraform(context.TODO(), clusterID.InfraID, installConfig); err != nil {
			return err
		}
	case typesopenstack.Name:
		if err := openstack.PreTerraform(); err != nil {
			return err
		}
	}

	tfvarsFiles := []*asset.File{}
	for _, file := range terraformVariables.Files() {
		tfvarsFiles = append(tfvarsFiles, file)
	}

	provider := infra.ProviderForPlatform(platform)
	files, err := provider.Provision(InstallDir, tfvarsFiles)
	c.FileList = append(c.FileList, files...) // append state files even in case of failure
	if err != nil {
		return fmt.Errorf("%s: %w", asset.ClusterCreationError, err)
	}

	return nil
}

func (c *Cluster) applyStage(platform string, stage terraform.Stage, terraformDir string, tfvarsFiles []*asset.File) (*asset.File, error) {
	// Copy the terraform.tfvars to a temp directory which will contain the terraform plan.
	tmpDir, err := os.MkdirTemp("", fmt.Sprintf("openshift-install-%s-", stage.Name()))
	if err != nil {
		return nil, errors.Wrap(err, "failed to create temp dir for terraform execution")
	}
	defer os.RemoveAll(tmpDir)

	var extraOpts []tfexec.ApplyOption
	for _, file := range tfvarsFiles {
		if err := os.WriteFile(filepath.Join(tmpDir, file.Filename), file.Data, 0o600); err != nil {
			return nil, err
		}
		extraOpts = append(extraOpts, tfexec.VarFile(filepath.Join(tmpDir, file.Filename)))
	}

	return c.applyTerraform(tmpDir, platform, stage, terraformDir, extraOpts...)
}

func (c *Cluster) applyTerraform(tmpDir string, platform string, stage terraform.Stage, terraformDir string, opts ...tfexec.ApplyOption) (*asset.File, error) {
	timer.StartTimer(stage.Name())
	defer timer.StopTimer(stage.Name())

	applyErr := terraform.Apply(tmpDir, platform, stage, terraformDir, opts...)

	// Write the state file to the install directory even if the apply failed.
	if data, err := os.ReadFile(filepath.Join(tmpDir, terraform.StateFilename)); err == nil {
		c.FileList = append(c.FileList, &asset.File{
			Filename: stage.StateFilename(),
			Data:     data,
		})
	} else if !os.IsNotExist(err) {
		logrus.Errorf("Failed to read tfstate: %v", err)
		return nil, errors.Wrap(err, "failed to read tfstate")
	}

	if applyErr != nil {
		return nil, errors.Wrap(applyErr, asset.ClusterCreationError)
	}

	outputs, err := terraform.Outputs(tmpDir, terraformDir)
	if err != nil {
		return nil, errors.Wrapf(err, "could not get outputs from stage %q", stage.Name())
	}

	outputsFile := &asset.File{
		Filename: stage.OutputsFilename(),
		Data:     outputs,
	}
	return outputsFile, nil
}
